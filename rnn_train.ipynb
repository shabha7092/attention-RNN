{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from rnn_attention import attention\n",
    "from tensorflow.contrib.rnn import GRUCell\n",
    "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE=100\n",
    "SEQUENCE_LENGTH=100\n",
    "EMBEDDING_DIM=128\n",
    "ATTENTION_SIZE=50\n",
    "KEEP_PROB=0.5\n",
    "vocabulary_size=5385\n",
    "num_epochs=5\n",
    "DELTA=0.5\n",
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path):\n",
    "    if data_path is None:\n",
    "        return None\n",
    "    data_files = glob.glob(os.path.join(data_path, 'part-*'))\n",
    "    data = pd.concat(map(lambda file: pd.read_csv(file, sep='#|,', header=None, engine='python'), data_files), axis = 0, ignore_index = True)\n",
    "    labels = np.array([data.iloc[:, 0].values])\n",
    "    sequences = np.array(data.iloc[:,1:].values)\n",
    "    return sequences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates mini batches\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed=0):\n",
    "    m = X.shape[0]                 \n",
    "    mini_batches = []\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :]\n",
    "    shuffled_Y = Y[:, permutation]\n",
    "\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size)    \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different Placeholders\n",
    "with tf.name_scope('Inputs'):\n",
    "    batch_ph = tf.placeholder(tf.int32, [None, SEQUENCE_LENGTH], name='batch_ph')\n",
    "    target_ph = tf.placeholder(tf.float32, [None], name='target_ph')\n",
    "    seq_len_ph = tf.placeholder(tf.int32, [None], name='seq_len_ph')\n",
    "    keep_prob_ph = tf.placeholder(tf.float32, name='keep_prob_ph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Layer\n",
    "with tf.name_scope('Embedding_layer'):\n",
    "    embeddings_var = tf.Variable(tf.random_uniform([vocabulary_size, EMBEDDING_DIM], -1.0, 1.0), trainable=True)\n",
    "    batch_embedded = tf.nn.embedding_lookup(embeddings_var, batch_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Layer\n",
    "with tf.name_scope('RNN'):\n",
    "    rnn_outputs, _ = bi_rnn(GRUCell(HIDDEN_SIZE), GRUCell(HIDDEN_SIZE), inputs=batch_embedded, sequence_length=seq_len_ph, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Layer\n",
    "with tf.name_scope('Attention_layer'):\n",
    "    attention_output, alphas = attention(rnn_outputs, ATTENTION_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Out\n",
    "with tf.name_scope('Drop_out'):\n",
    "    drop = tf.nn.dropout(attention_output, keep_prob_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully Connected Layer\n",
    "with tf.name_scope('Fully_connected_layer'):\n",
    "    hidden_size = attention_output.shape[1].value\n",
    "    W = tf.Variable(tf.truncated_normal([hidden_size, 1], stddev=0.1))  # Hidden size is multiplied by 2 for Bi-RNN\n",
    "    b = tf.Variable(tf.constant(0., shape=[1]))\n",
    "    y_hat = tf.nn.xw_plus_b(drop, W, b)\n",
    "    y_hat = tf.squeeze(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "with tf.name_scope('Metrics'):\n",
    "    # Cross-entropy loss and optimizer initialization\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=target_ph))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)\n",
    "    # Accuracy metric\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_hat)), target_ph), tf.float32))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSeqLength(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    if 0 in list(x):\n",
    "        return list(x).index(0) + 1\n",
    "    return len(list(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "[0.43063702672668785, 0.4112151782378957, 0.4044541634321028, 0.3977318465433289, 0.39048972000027243]\n",
      "[array([0.        , 0.        , 0.        , ..., 0.99942385, 0.99942385,\n",
      "       1.        ]), array([0.        , 0.        , 0.        , ..., 0.99810307, 0.99810981,\n",
      "       1.        ]), array([0.        , 0.        , 0.        , ..., 0.99776277, 0.99776951,\n",
      "       1.        ]), array([0.        , 0.        , 0.        , ..., 0.99367916, 0.9936859 ,\n",
      "       1.        ]), array([0.        , 0.        , 0.        , ..., 0.99028289, 0.99028963,\n",
      "       1.        ])]\n",
      "[array([0.00000000e+00, 3.38162291e-06, 1.52173031e-03, ...,\n",
      "       9.99996618e-01, 1.00000000e+00, 1.00000000e+00]), array([0.00000000e+00, 3.38162291e-06, 6.11735584e-03, ...,\n",
      "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00]), array([0.00000000e+00, 3.38162291e-06, 7.92652410e-03, ...,\n",
      "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00]), array([0.00000000e+00, 3.38162291e-06, 3.29031909e-03, ...,\n",
      "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00]), array([0.00000000e+00, 3.38162291e-06, 3.19225203e-03, ...,\n",
      "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00])]\n",
      "[array([ 12.722765,  11.722765,   9.139218, ..., -13.605996, -13.618851,\n",
      "       -16.71028 ], dtype=float32), array([ 13.735609 ,  12.735609 ,   8.795131 , ..., -13.6281805,\n",
      "       -13.630135 , -17.673811 ], dtype=float32), array([ 15.300789,  14.300789,   9.217754, ..., -15.278155, -15.278327,\n",
      "       -19.72753 ], dtype=float32), array([ 16.679306,  15.679306,  10.793094, ..., -16.41643 , -16.416563,\n",
      "       -22.695383], dtype=float32), array([ 19.496252,  18.496252,  11.98626 , ..., -18.01429 , -18.015818,\n",
      "       -27.638422], dtype=float32)]\n",
      "[0.8706715514350178, 0.8828328722524974, 0.886944566987998, 0.8910475165328273, 0.8953913687059925]\n",
      "########################\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sequences_train, labels_train = read_data('data/train')\n",
    "    sequences_test, labels_test = read_data('data/test')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    epoch_loss = []\n",
    "    epoch_thresholds = []\n",
    "    epoch_fpr = []\n",
    "    epoch_tpr = []\n",
    "    epoch_auc = []\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_list = []\n",
    "        pred_list = []\n",
    "        lab_list = []\n",
    "        loss_train = 0\n",
    "        loss_test = 0\n",
    "        minibatches = random_mini_batches(sequences_train, labels_train)\n",
    "        for minibatch in minibatches:\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "            #seq_len = np.array([list(x).index(0) + 1 for x in minibatch_X])\n",
    "            seq_len = np.array([getSeqLength(x) for x in minibatch_X])\n",
    "            feed_dict = {batch_ph: minibatch_X,\n",
    "                         target_ph: minibatch_Y.flatten('C'),\n",
    "                         seq_len_ph: seq_len,\n",
    "                         keep_prob_ph: KEEP_PROB}\n",
    "            _, loss_tr, pred, lab = sess.run([optimizer, loss, y_hat, target_ph], feed_dict=feed_dict)\n",
    "            loss_train = loss_tr * DELTA + loss_train * (1 - DELTA)\n",
    "            loss_list.append(loss_train)\n",
    "            pred_list += list(pred)\n",
    "            lab_list  += list(lab)\n",
    "        loss_train = sum(loss_list)/len(loss_list)\n",
    "        fpr_train, tpr_train, thresholds_train = metrics.roc_curve(lab_list, pred_list)\n",
    "        auc_train = metrics.auc(fpr_train, tpr_train)\n",
    "        epoch_loss.append(loss_train)\n",
    "        epoch_thresholds.append(thresholds_train)\n",
    "        epoch_fpr.append(fpr_train)\n",
    "        epoch_tpr.append(tpr_train)\n",
    "        epoch_auc.append(auc_train)\n",
    "    \n",
    "    print('########################')\n",
    "    print(epoch_loss)\n",
    "    print(epoch_fpr)\n",
    "    print(epoch_tpr)\n",
    "    print(epoch_thresholds)\n",
    "    print(epoch_auc)\n",
    "    print('########################')\n",
    "#         loss_list = []\n",
    "#         lab_list = []\n",
    "#         pred_list = []\n",
    "#         sig_acts = []\n",
    "#         emb_list = []\n",
    "#         act_scores = []\n",
    "#         all_activities = []\n",
    "#         minibatches = random_mini_batches(sequences_test, labels_test)\n",
    "#         for minibatch in minibatches:\n",
    "#             (minibatch_X, minibatch_Y) = minibatch\n",
    "#             seq_len = np.array([list(x).index(0) + 1 for x in minibatch_X])\n",
    "#             feed_dict = {batch_ph: x_batch,\n",
    "#                          target_ph: y_batch,\n",
    "#                          seq_len_ph: seq_len,\n",
    "#                          keep_prob_ph: 1.0\n",
    "#                         }\n",
    "#             loss_te, lab, pred, alp, u_emb = sess.run([loss, target_ph, y_hat, alphas, attention_output],\n",
    "#                                                                  feed_dict=feed_dict)\n",
    "#             emb_list += list(u_emb)                        \n",
    "#             pred_list += list(pred)\n",
    "#             lab_list += list(lab)\n",
    "#             loss_list.append(loss_te)\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
